{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtOyvQfNSZBuBq2KI3eSjm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mainabryan/100-days-challenge-for-machine-learning-practise/blob/main/WEB_SCRAPING_(CHECKPOINT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective is to automate the extraction of HTML content, article titles, text, and internal links from Wikipedia pages into a consolidated function that accepts any Wikipedia URL for efficient data retrieval and processing.\n",
        "\n",
        "\n",
        "Instructions\n",
        "Create a Python script to automate data extraction from Wikipedia pages. The script will retrieve HTML content, extract article titles and text, collect internal links, and consolidate these tasks into one function that accepts a Wikipedia URL. This will be tested on a specific Wikipedia page to validate functionality.\n",
        "\n",
        "1) Write a function to Get and parse html content from a Wikipedia page\n",
        "\n",
        "2) Write a function to Extract article title\n",
        "\n",
        "3) Write a function to Extract article text for each paragraph with their respective\n",
        "\n",
        "headings. Map those headings to their respective paragraphs in the dictionary.\n",
        "\n",
        "4) Write a function to collect every link that redirects to another Wikipedia page\n",
        "\n",
        "5) Wrap all the previous functions into a single function that takes as parameters a Wikipedia link\n",
        "\n",
        "6) Test the last function on a Wikipedia page of your choice"
      ],
      "metadata": {
        "id": "fjK7sS9pX5iq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ni2wLKTnYcOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 write a function to Get and parse html contenet from a wikipedia page"
      ],
      "metadata": {
        "id": "QTfpV5oBYB8s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2njaXAmzXmix"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_soup(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2) Write a function to Extract article title"
      ],
      "metadata": {
        "id": "nD7d4QVoYeJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_title(soup):\n",
        "    return soup.find('h1', id='firstHeading').get_text()\n"
      ],
      "metadata": {
        "id": "uKmpYWoCYdOM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3) Write a function to Extract article text for each paragraph with their respective"
      ],
      "metadata": {
        "id": "WdpOpDv_Y6Wk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_by_heading(soup):\n",
        "    content = soup.find('div', class_='mw-parser-output')\n",
        "    data = {}\n",
        "    current_heading = 'Introduction'\n",
        "\n",
        "    for tag in content.find_all(['h2', 'h3', 'p']):\n",
        "        if tag.name in ['h2', 'h3']:\n",
        "            heading_text = tag.get_text().strip().replace('[edit]', '')\n",
        "            current_heading = heading_text\n",
        "        elif tag.name == 'p':\n",
        "            paragraph = tag.get_text().strip()\n",
        "            if current_heading not in data:\n",
        "                data[current_heading] = []\n",
        "            data[current_heading].append(paragraph)\n",
        "\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "a0gQ7NHkY7l2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Write a function to collect every link that redirects to another Wikipedia page\n",
        "\n"
      ],
      "metadata": {
        "id": "q1F-ibQOZILQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_internal_links(soup):\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.startswith('/wiki/') and ':' not in href:\n",
        "            full_url = \"https://en.wikipedia.org\" + href\n",
        "            links.append(full_url)\n",
        "    return list(set(links))  # remove duplicates\n"
      ],
      "metadata": {
        "id": "GPeokw11ZHK-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4) Write a function to collect every link that redirects to another Wikipedia page"
      ],
      "metadata": {
        "id": "ORQsqgtHZS7_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_internal_links(soup):\n",
        "    links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.startswith('/wiki/') and ':' not in href:\n",
        "            full_url = \"https://en.wikipedia.org\" + href\n",
        "            links.append(full_url)\n",
        "    return list(set(links))"
      ],
      "metadata": {
        "id": "dBf_TburZh_y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5) Wrap all the previous functions into a single function that takes as parameters a Wikipedia link"
      ],
      "metadata": {
        "id": "GRmGYN9KZn3h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_wikipedia(url):\n",
        "    soup = get_soup(url)\n",
        "    title = get_title(soup)\n",
        "    content_by_heading = extract_text_by_heading(soup)\n",
        "    internal_links = get_internal_links(soup)\n",
        "\n",
        "    return {\n",
        "        \"title\": title,\n",
        "        \"sections\": content_by_heading,\n",
        "        \"internal_links\": internal_links\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Fve5PQrbZu4Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ✅ Step 6: Test It\n",
        "\n"
      ],
      "metadata": {
        "id": "CLzMPgRJZzzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = scrape_wikipedia(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
        "\n",
        "print(\"Title:\", result[\"title\"])\n",
        "print(\"\\nSections:\")\n",
        "for heading, paragraphs in result[\"sections\"].items():\n",
        "    print(f\"\\n{heading} ({len(paragraphs)} paragraphs)\")\n",
        "    for para in paragraphs[:1]:  # Print only first paragraph of each\n",
        "        print(\" -\", para[:100], \"...\")\n",
        "print(f\"\\nFound {len(result['internal_links'])} internal links.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHCsfOhsZ7W0",
        "outputId": "dad48331-fad7-4d70-b4ed-301552d84b8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Machine learning\n",
            "\n",
            "Sections:\n",
            "\n",
            "Introduction (5 paragraphs)\n",
            " -  ...\n",
            "\n",
            "History (5 paragraphs)\n",
            " - The term machine learning was coined in 1959 by Arthur Samuel, an IBM employee and pioneer in the fi ...\n",
            "\n",
            "Artificial intelligence (3 paragraphs)\n",
            " - As a scientific endeavour, machine learning grew out of the quest for artificial intelligence (AI).  ...\n",
            "\n",
            "Data compression (6 paragraphs)\n",
            " - There is a close connection between machine learning and compression. A system that predicts the pos ...\n",
            "\n",
            "Data mining (2 paragraphs)\n",
            " - Machine learning and data mining often employ the same methods and overlap significantly, but while  ...\n",
            "\n",
            "Generalization (1 paragraphs)\n",
            " - Characterizing the generalisation of various learning algorithms is an active topic of current resea ...\n",
            "\n",
            "Statistics (4 paragraphs)\n",
            " - Machine learning and statistics are closely related fields in terms of methods, but distinct in thei ...\n",
            "\n",
            "Statistical physics (1 paragraphs)\n",
            " - Analytical and computational techniques derived from deep-rooted physics of disordered systems can b ...\n",
            "\n",
            "Theory (4 paragraphs)\n",
            " - A core objective of a learner is to generalise from its experience.[5][43] Generalisation in this co ...\n",
            "\n",
            "Approaches (3 paragraphs)\n",
            " -  ...\n",
            "\n",
            "Supervised learning (3 paragraphs)\n",
            " - Supervised learning algorithms build a mathematical model of a set of data that contains both the in ...\n",
            "\n",
            "Unsupervised learning (3 paragraphs)\n",
            " - Unsupervised learning algorithms find structures in data that has not been labelled, classified or c ...\n",
            "\n",
            "Semi-supervised learning (2 paragraphs)\n",
            " - Semi-supervised learning falls between unsupervised learning (without any labelled training data) an ...\n",
            "\n",
            "Reinforcement learning (1 paragraphs)\n",
            " - Reinforcement learning is an area of machine learning concerned with how software agents ought to ta ...\n",
            "\n",
            "Dimensionality reduction (1 paragraphs)\n",
            " - Dimensionality reduction is a process of reducing the number of random variables under consideration ...\n",
            "\n",
            "Other types (18 paragraphs)\n",
            " - Other approaches have been developed which do not fit neatly into this three-fold categorisation, an ...\n",
            "\n",
            "Models (2 paragraphs)\n",
            " - A machine learning model is a type of mathematical model that, once \"trained\" on a given dataset, ca ...\n",
            "\n",
            "Artificial neural networks (4 paragraphs)\n",
            " - Artificial neural networks (ANNs), or connectionist systems, are computing systems vaguely inspired  ...\n",
            "\n",
            "Decision trees (1 paragraphs)\n",
            " - Decision tree learning uses a decision tree as a predictive model to go from observations about an i ...\n",
            "\n",
            "Random forest regression (1 paragraphs)\n",
            " - Random forest regression (RFR) falls under umbrella of decision tree-based models. RFR is an ensembl ...\n",
            "\n",
            "Support-vector machines (1 paragraphs)\n",
            " - Support-vector machines (SVMs), also known as support-vector networks, are a set of related supervis ...\n",
            "\n",
            "Regression analysis (2 paragraphs)\n",
            " - Regression analysis encompasses a large variety of statistical methods to estimate the relationship  ...\n",
            "\n",
            "Bayesian networks (1 paragraphs)\n",
            " - A Bayesian network, belief network, or directed acyclic graphical model is a probabilistic graphical ...\n",
            "\n",
            "Gaussian processes (3 paragraphs)\n",
            " - A Gaussian process is a stochastic process in which every finite collection of the random variables  ...\n",
            "\n",
            "Genetic algorithms (1 paragraphs)\n",
            " - A genetic algorithm (GA) is a search algorithm and heuristic technique that mimics the process of na ...\n",
            "\n",
            "Belief functions (1 paragraphs)\n",
            " - The theory of belief functions, also referred to as evidence theory or Dempster–Shafer theory, is a  ...\n",
            "\n",
            "Rule-based models (1 paragraphs)\n",
            " - Rule-based machine learning (RBML) is a branch of machine learning that automatically discovers and  ...\n",
            "\n",
            "Training models (2 paragraphs)\n",
            " - Typically, machine learning models require a high quantity of reliable data to perform accurate pred ...\n",
            "\n",
            "Applications (5 paragraphs)\n",
            " - There are many applications for machine learning, including: ...\n",
            "\n",
            "Limitations (4 paragraphs)\n",
            " - Although machine learning has been transformative in some fields, machine-learning programs often fa ...\n",
            "\n",
            "Explainability (1 paragraphs)\n",
            " - Explainable AI (XAI), or Interpretable AI, or Explainable Machine Learning (XML), is artificial inte ...\n",
            "\n",
            "Overfitting (1 paragraphs)\n",
            " - Settling on a bad, overly complex theory gerrymandered to fit all the past training data is known as ...\n",
            "\n",
            "Other limitations and vulnerabilities (3 paragraphs)\n",
            " - Learners can also disappoint by \"learning the wrong lesson\". A toy example is that an image classifi ...\n",
            "\n",
            "Model assessments (2 paragraphs)\n",
            " - Classification of machine learning models can be validated by accuracy estimation techniques like th ...\n",
            "\n",
            "Ethics (2 paragraphs)\n",
            " -  ...\n",
            "\n",
            "Bias (6 paragraphs)\n",
            " - Different machine learning approaches can suffer from different data biases. A machine learning syst ...\n",
            "\n",
            "Financial incentives (1 paragraphs)\n",
            " - There are concerns among health care professionals that these systems might not be designed in the p ...\n",
            "\n",
            "Hardware (1 paragraphs)\n",
            " - Since the 2010s, advances in both machine learning algorithms and computer hardware have led to more ...\n",
            "\n",
            "Tensor Processing Units (TPUs) (1 paragraphs)\n",
            " - Tensor Processing Units (TPUs) are specialised hardware accelerators developed by Google specificall ...\n",
            "\n",
            "Neuromorphic computing (2 paragraphs)\n",
            " - Neuromorphic computing refers to a class of computing systems designed to emulate the structure and  ...\n",
            "\n",
            "Embedded machine learning (1 paragraphs)\n",
            " - Embedded machine learning is a sub-field of machine learning where models are deployed on embedded s ...\n",
            "\n",
            "Software (1 paragraphs)\n",
            " - Software suites containing a variety of machine learning algorithms include the following: ...\n",
            "\n",
            "Found 907 internal links.\n"
          ]
        }
      ]
    }
  ]
}